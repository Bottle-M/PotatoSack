# PotatoSack 备份机制设计

## 云端备份存储结构

### 大概的目录结构

```bash
AppFolder
├── 020240104000001
│   ├── backup.json
│   ├── full.zip
│   ├── incre000001.zip
│   ├── incre000002.zip
│   ├── _world.json
│   ├── _world_nether.json
│   └── _world_the_end.json
└── 020240104000002 # 一组备份
    ├── backup.json # 备份记录文件
    ├── full.zip # 存放所有指定世界的全量备份
    ├── incre000001.zip # 存放所有指定世界的增量备份
    ├── incre000002.zip
    ├── incre000003.zip
    ├── _world.json # 存放world世界目录下所有文件的修改情况（最后的MD5哈希）
    ├── _world_nether.json
    └── _world_the_end.json
```

`full.zip`存放的就是所有指定世界的全量备份。

### backup.json

1. 记录上次**全量备份**的时间戳；
2. 记录上次**增量备份**的时间戳；
3. 记录此文件更新的时间戳。
4. 最后一组备份的组号，如`020240104000002`。
5. 上一次增量备份的序号，如`000001`。

### _世界名.json

文件名前加一个下划线是为了防止有世界叫 "`backup`"，和 `backup.json` 冲突。

1. 记录某个世界数据目录中的所有文件的最后哈希值。
   * 键值对: `<相对于服务端根目录的路径, MD5 哈希值>`
2. 记录此文件的更新时间戳。

文件 `_world.json` 示例如下:  

```json
{
    "fileUpdateTime": 1718853394,
    "lastFileHashes": {
        "world/playerdata/911cb843-480d-4c4c-80e2-1d4b1234ab68.dat_old": "8e61dc5106573480912f8f1f73c050ba",
        "world/entities/r.-7.3.mca": "ded5a6b1fa64387e4b05836a686c6e11",
        "...": "..."
    }
}
```

### incre*.zip

存放指定世界的增量备份，压缩包内目录结构如下：

```bash
incre*.zip # 压缩包内
├── deleted.files # 标记相比上次增量备份，被删除的文件
├── world # 世界数据
│   ├── level.dat
│   ├── level.dat_old
│   ├── paper-world.yml
│   ├── region
│   │  ├── r.0.-1.mca
│   │  ├── r.0.0.mca
│   │  ├── r.0.1.mca
│   │  └── ...
│   └── ...
├── world_nether # 世界数据
│   ├── level.dat
│   ├── session.lock
│   └── ...
└── world_the_end # 世界数据
    ├── level.dat
    └── ...
```

为了将被删除的文件考虑在内，这里还设置了一个`deleted.files`来进行标记。

`deleted.files`文件中**每行记录一个**被删除的文件的相对路径（相对于服务端根目录）。

## 从云端拉取备份记录文件

1. 对`AppFolder`进行列表。
2. 按照字典序进行降序排序，找到最新的**一组备份**。（比如上面的`020240104000002`）
3. 如果这一组备份是当天的（比如`020240104000002`对应2024年1月4日），则从目录中下载`backup.json`、`_世界名.json`到本地。
4. 如果**相应记录文件不存在**或**这一组备份并不是当天建立**的，则[新建一组备份](#新建一组备份)。

## 记录文件在本地存放的位置

存放在插件目录的`data`子目录下，比如`backup.json`应该存放在`plugins/PotatoSack/data/backup.json`。

## 插件启动时

1. 检查本地（插件目录）是否有备份记录文件`backup.json`以及`_世界名.json`
   ，如果没有则会[从云端尝试拉取](#从云端拉取备份记录文件)对应的备份记录文件`backup.json`和`_世界名.json`。
    - 一个备份记录文件对应**一组备份**
    - 本地只存储当前一组备份的记录

2. 设立计时器，用于实现定时进行备份。计时器通过`backup.json`中的时间戳来判断是否需要进行备份。

## 新建一组备份

1. 移除本地备份记录文件`backup.json`和`_世界名.json`。
2. 进行一次全量备份，重新生成备份记录文件`backup.json`和`_世界名.json`。
3. 将第2步产生的文件传输到云端。
4. 删除过时的备份组。

## 进行增量备份

比如对`world`世界进行增量备份:

1. 读取`world.json`中存放的所有文件的最后md5哈希值`md5Set1`。
2. 扫描`world`目录中的所有文件，得到所有文件的最后md5哈希值`md5Set2`
3. 计算`md5Set1`和`md5Set2`的差集`diffSet`，差集中的是被删除的文件，将其写入`deleted.files`文件中。
4. 将有更新的文件打包成`incre*.zip`，更新`backup.json`和`world.json`。
5. 将第4步产生的文件传输到云端。

## 流式压缩上传 

* 2024.2.18 新增  

这天突然想到一个经常遇到的问题，对于一些小服主（比如我）来说，常常会租面板服来搭建朋友间一起玩的小服务器。  

有些面板服可能会在硬盘资源上比较吝啬（其实即使 VPS 也是如此），比如限制分配给服务器容器的硬盘空间上限为 10 GiB，这种时候，如果服务器存档**在压缩后**还有 5 GiB 以上，则插件肯定没法先把生成的 `zip` 文件全写入临时目录里，然后再上传。（而这是本插件的默认备份上传策略）  

解决方法呢，其实就是把 `ZipOutputStream` 生成的数据直接上传到云端。  

只需要在内存中维护一个 `CHUNK_SIZE` （大文件分块上传的每块大小）大小的缓冲区，让 zip 流输出到这个缓冲区里，一旦满了就把这一块立即上传到云端，不停复用这一块缓冲区。  

* **问题**：OneDrive 上传大文件前必须要知道**确切的**文件大小，而这里 zip 流的输出是和上传几乎同时进行的，怎么确定生成的 zip 文件总大小？  
* **解决**：时间换空间。先跑一遍压缩过程，对于生成的每个字节仅计数，随即抛弃，这样就可以算出来较为确切的 zip 文件总大小。然后再进行分块压缩上传的操作。（目前好像也只有这种解决方案了，代价是需要 CPU 跑两趟）  
* **注意**：需要严格保证两次 `ZipOutputStream` 输出的字节数一致，因此在备份过程中需要**临时停止 Minecraft 世界自动保存**。  
* 即使停止了自动保存也有可能两次 `ZipOutputStream` 输出的字节数不一致，因此本插件还配备了一些重试机制。